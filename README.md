# Distilling Datasets through Agreement-Aware Learning (DIVA)

This is an official PyTorch implementation of the paper **Distilling Datasets through Agreement-Aware Learning**. In this work, we:

- We propose a novel framework, Distilling Datasets through Agreement-Aware Learning (DIVA), which integrates multiple model perspectives to synthesize a distilled dataset that
encapsulates rich features and produces high-quality soft
labels by batch-specific normalization.

- By integrating recent advancements, refining framework
design and optimization techniques, we establish a strong
baseline within DIVA framework that already achieves
state-of-the-art performance in dataset distillation.

- Through experiments across multiple datasets, we demonstrate that DIVA improves generalization, mitigates overfitting, and outperforms prior methods in various datalimited scenarios, highlighting its effectiveness as a scalable and reliable solution for dataset distillation.